
# ---------------------------------------------------------------
#  Deep-Survival Pipeline for Liver ROI
# ---------------------------------------------------------------
#  Goal
#  ----
#  Train and evaluate a 3-D CNN + DeepSurv model that predicts
#  Hepatocellular Disease-Free Survival (HDFS) from liver CT volumes.
#
#  Customization Choices
#  ---------------------
#  • All file-system paths, variable names, and printed messages assume
#    the Region-of-Interest (ROI) is the **Liver**.  
#  • Comments were expanded throughout to clarify *why* each block
#    exists and *how* it contributes to the end-to-end liver workflow.  
#  • No author names or calendar dates appear anywhere in this file.  
#  • Section banners (`##############################################################################`)
#    make it easy to navigate the code.  
#
#  Usage
#  -----
#  Adjust the three absolute directory paths under “Paths” if your data
#  lives elsewhere, then run:
#
#      python liver_hdfs_pipeline.py
#
#  ---------------------------------------------------------------
#  Imports
#  ---------------------------------------------------------------

import os
import warnings

# Silence noisy warnings — useful when training in a notebook / cluster log.
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

import random
import pickle
import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
import scipy.ndimage
from skimage.transform import resize

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
from sksurv.util import Surv
from sksurv.metrics import concordance_index_ipcw

from copy import deepcopy

# Mixed-precision helpers
from torch.cuda.amp import autocast, GradScaler

# Data loading / augmentations
from torch.utils.data import DataLoader, SubsetRandomSampler, WeightedRandomSampler
from torchvision import transforms
import nibabel as nib

# Pretty logging
from rich.console import Console
from rich.progress import Progress, BarColumn, TextColumn, TaskProgressColumn

console = Console(style="white")

##############################################################################
# Paths (edit these three if your folder structure differs)
##############################################################################
cached_dir   = "/mnt/largedrive1/rmojtahedi/Deep_Survival/Final_Version/Liver/Liver_HDFS"
results_dir  = "/mnt/largedrive1/rmojtahedi/Deep_Survival/Final_Version/Liver/Liver_HDFS"
os.makedirs(results_dir, exist_ok=True)

liver_image_dir   = "/mnt/largedrive1/rmojtahedi/KSPIE/liver/ct"              # CT volumes
liver_label_dir   = "/mnt/largedrive1/rmojtahedi/KSPIE/liver/final_seg_split" # Binary liver masks
clinical_csv_path = "/mnt/largedrive0/rmojtahedi/Kaitlyn_SPIE/Deep_Survival/npj_Digital_Medicine_Clinical_Data_FINAL.csv"

# GPU selection: by default grab GPU 1 if available.
device      = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
scaler_amp  = GradScaler(enabled=(device.type == 'cuda'))

##############################################################################
# Reproducibility helpers
##############################################################################
EVAL_SEED = 999   # Stable seed for TTA + MC-dropout evaluation

def set_seed(seed: int):
    """Typical global seed setter for *training*."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def set_eval_seed(seed: int):
    """Use a fixed seed inside inference loops (TTA / bootstrap)."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

##############################################################################
# Match CT volumes, segmentation masks, and clinical rows
##############################################################################
def get_core_id(filename: str) -> str:
    """Strip NIfTI extension (.nii / .nii.gz) to yield core ID."""
    if filename.endswith(".nii.gz"):
        return filename[:-7]
    elif filename.endswith(".nii"):
        return filename[:-4]
    return filename

def get_candidate_id(name: str) -> str:
    """
    Clinical CSV uses IDs without the last underscore-suffix.
    This helper lets us map CT filenames ➔ candidate clinical IDs.
    """
    parts = name.split('_')
    return '_'.join(parts[:-1]) if len(parts) > 1 else name

# Collect file basenames
image_files = os.listdir(liver_image_dir)
label_files = os.listdir(liver_label_dir)

image_names = {get_core_id(fn) for fn in image_files}
label_names = {get_core_id(fn) for fn in label_files}

# Check for I/O mismatches (helps catch typos in dataset assembly)
unmatched_images = sorted(list(image_names - label_names))
unmatched_labels = sorted(list(label_names - image_names))

common_names     = image_names.intersection(label_names)
candidate_pairs  = [(name, get_candidate_id(name)) for name in sorted(common_names)]

# Load clinical table
clinical_data_full = pd.read_csv(clinical_csv_path)
clinical_data_full = clinical_data_full.dropna(subset=["XNAT ID"])
clinical_data_full["XNAT ID"] = clinical_data_full["XNAT ID"].astype(str)
clinical_ids = set(clinical_data_full["XNAT ID"])

# Build (image-label) ↔ clinical matches
final_matched = []
unmatched_candidates = []
for image_label, candidate in candidate_pairs:
    if candidate in clinical_ids:
        final_matched.append((image_label, candidate))
    else:
        unmatched_candidates.append((image_label, candidate))

# Manual overrides for edge-case IDs that differ between imaging and CSV
manual_mapping = {
    # —— Add / edit entries here if your dataset has additional anomalies ——
    "RIA_17-010A_000_202": "RIA_17-010A_000_202",
    "RIA_17-010A_000_439": "RIA_17-010A_000_439",
    "RIA_17-010A_002_118": "RIA_17-010A_002_118",
    "RIA_17-010A_002_131": "RIA_17-010A_002_131",
    "RIA_17-010A_002_159": "RIA_17-010A_002_159",
    "RIA_17-010A_002_170": "RIA_17-010A_002_170",
    # double-underscore cases below come from duplicate columns in CSV
    "RIA_17-010B_000_010": "RIA_17-010B_000_RIA_17-010B_000_010",
    "RIA_17-010B_000_054": "RIA_17-010B_000_RIA_17-010B_000_054",
    "RIA_17-010B_000_064": "RIA_17-010B_000_RIA_17-010B_000_064",
    "RIA_17-010B_000_123": "RIA_17-010B_000_RIA_17-010B_000_123",
    "RIA_17-010B_000_159": "RIA_17-010B_000_RIA_17-010B_000_159",
    "RIA_17-010B_000_165": "RIA_17-010B_000_RIA_17-010B_000_165",
    "RIA_17-010B_000_170": "RIA_17-010B_000_RIA_17-010B_000_170",
    "RIA_17-010B_000_200": "RIA_17-010B_000_RIA_17-010B_000_200",
    "RIA_17-010B_000_209": "RIA_17-010B_000_RIA_17-010B_000_209",
    "RIA_17-010B_000_258": "RIA_17-010B_000_RIA_17-010B_000_258",
}

manually_matched = []
still_unmatched  = []
for image_label, candidate in unmatched_candidates:
    if image_label in manual_mapping:
        corrected = manual_mapping[image_label]
        if corrected in clinical_ids:
            manually_matched.append((image_label, corrected))
        else:
            still_unmatched.append((image_label, candidate))
    else:
        still_unmatched.append((image_label, candidate))

final_matched.extend(manually_matched)
matched_xnat_set       = {cid for _, cid in final_matched}
unmatched_clinical_ids = sorted(list(clinical_ids - matched_xnat_set))

# Merge matched imaging rows with survival labels
matched_df  = pd.DataFrame(final_matched, columns=["Image_Label_Base", "XNAT ID"])
merged_data = pd.merge(matched_df, clinical_data_full, on="XNAT ID", how="inner")

# Drop unusable rows (missing time / censor flags or non-positive durations)
merged_data = merged_data.dropna(subset=["Surv Time Censor 4", "Censor 4: HDFS", "HDFSTrain"])
merged_data = merged_data[merged_data["Surv Time Censor 4"] > 0]

# Train/validation split was pre-assigned via "HDFSTrain" flag in CSV
train_rows = merged_data[merged_data["HDFSTrain"] == 1]
val_rows   = merged_data[merged_data["HDFSTrain"] == 0]

train_image_names = train_rows["Image_Label_Base"].tolist()
val_image_names   = val_rows["Image_Label_Base"].tolist()

##############################################################################
# Data-augmentation transforms (3-D, liver-specific)
##############################################################################
class RandomGaussianNoise:
    """Add light Gaussian noise to simulate scanner variability."""
    def __init__(self, p=0.5, std_range=(0.01, 0.03)):
        self.p = p
        self.std_range = std_range
    def __call__(self, volume):
        if random.random() < self.p:
            std   = random.uniform(*self.std_range)
            noise = np.random.randn(*volume.shape) * std
            volume = volume + noise
        return volume

class RandomZoom3D:
    """Isotropic zoom-in / zoom-out while maintaining spatial size."""
    def __init__(self, p=0.5, zoom_range=(0.9, 1.1)):
        self.p = p
        self.zoom_range = zoom_range
    def __call__(self, volume):
        if random.random() < self.p:
            zoom_factor = random.uniform(*self.zoom_range)
            zoomed = scipy.ndimage.zoom(volume, zoom_factor, order=1)
            orig_shape = volume.shape
            # Center-crop or zero-pad back to original shape
            if zoomed.shape[0] >= orig_shape[0]:
                sx = (zoomed.shape[0] - orig_shape[0]) // 2
                sy = (zoomed.shape[1] - orig_shape[1]) // 2
                sz = (zoomed.shape[2] - orig_shape[2]) // 2
                ex, ey, ez = sx+orig_shape[0], sy+orig_shape[1], sz+orig_shape[2]
                volume = zoomed[sx:ex, sy:ey, sz:ez]
            else:
                padded = np.zeros(orig_shape, dtype=zoomed.dtype)
                ox = (orig_shape[0] - zoomed.shape[0]) // 2
                oy = (orig_shape[1] - zoomed.shape[1]) // 2
                oz = (orig_shape[2] - zoomed.shape[2]) // 2
                padded[ox:ox+zoomed.shape[0],
                       oy:oy+zoomed.shape[1],
                       oz:oz+zoomed.shape[2]] = zoomed
                volume = padded
        return volume

class RandomRotate3D:
    """Small random 3-D rotations about a random axis pair."""
    def __init__(self, p=0.5, max_angle=10):
        self.p = p
        self.max_angle = max_angle
    def __call__(self, volume):
        if random.random() < self.p:
            axis_pairs = [(0,1), (0,2), (1,2)]
            axes  = random.choice(axis_pairs)
            angle = random.uniform(-self.max_angle, self.max_angle)
            volume = scipy.ndimage.rotate(volume, angle, axes=axes,
                                          reshape=False, order=1)
        return volume

class RandomFlip3D:
    """50 % chance to flip along a random axis (x, y, or z)."""
    def __init__(self, p=0.5):
        self.p = p
    def __call__(self, volume):
        if random.random() < self.p:
            flip_axis = random.choice([0, 1, 2])
            volume = np.flip(volume, axis=flip_axis).copy()
        return volume

class RandomIntensityShift:
    """CT intensity bias shift (mimics different calibrations)."""
    def __init__(self, p=0.5, shift_range=(-0.07, 0.07)):
        self.p = p
        self.shift_range = shift_range
    def __call__(self, volume):
        if random.random() < self.p:
            shift = random.uniform(*self.shift_range)
            volume = volume + shift
        return volume

class RandomIntensityScale:
    """Global scaling (contrast variation)."""
    def __init__(self, p=0.5, scale_range=(0.95, 1.05)):
        self.p = p
        self.scale_range = scale_range
    def __call__(self, volume):
        if random.random() < self.p:
            sc_ = random.uniform(*self.scale_range)
            volume = volume * sc_
        return volume

class RandomGamma:
    """Gamma correction augmentation."""
    def __init__(self, p=0.5, gamma_range=(0.9, 1.1)):
        self.p = p
        self.gamma_range = gamma_range
    def __call__(self, volume):
        if random.random() < self.p:
            gm = random.uniform(*self.gamma_range)
            volume = np.clip(volume, 0, None)
            vmin, vmax = volume.min(), volume.max()
            if vmax > vmin:
                normv = (volume - vmin) / (vmax - vmin + 1e-10)
                normv = normv ** gm
                volume = normv * (vmax - vmin) + vmin
        return volume

# Compose final augmentation pipeline
augment_transform = transforms.Compose([
    RandomRotate3D(p=0.5, max_angle=10),
    RandomFlip3D(p=0.5),
    RandomZoom3D(p=0.5, zoom_range=(0.9, 1.1)),
    RandomGaussianNoise(p=0.5, std_range=(0.01, 0.03)),
    RandomIntensityShift(p=0.5, shift_range=(-0.07, 0.07)),
    RandomIntensityScale(p=0.5, scale_range=(0.95, 1.05)),
    RandomGamma(p=0.5, gamma_range=(0.9, 1.1))
])

##############################################################################
# NPY cache builder
##############################################################################
def create_np_data(image_dir, label_dir, clinical_df,
                   train_list, val_list,
                   npy_save_path, shape=(64, 64, 64)):
    """
    Convert NIfTI volumes → NumPy arrays, apply liver masks,
    z-score them *across the entire training+validation cohort*,
    and save to a single `.npy` dict for fast reuse.
    """
    console.print("Starting conversion to NPY...")

    all_pids = train_list + val_list
    gather_samples = []

    # Pass 1: Build global StandardScaler on masked voxels
    with Progress(
        TextColumn("{task.description}", style="black", justify="center"),
        BarColumn(bar_width=None, complete_style="green", finished_style="green"),
        TaskProgressColumn(),
        TextColumn("{task.completed}/{task.total} done", style="black", justify="center"),
        console=Console(),
        transient=True
    ) as progress:
        task_id = progress.add_task("Loading and resizing", total=len(all_pids))
        for pid in all_pids:
            row = clinical_df[clinical_df["Image_Label_Base"] == pid]
            if len(row) == 0:
                # Clinical row missing — skip (will warn later)
                progress.update(task_id, advance=1)
                continue

            # Try .nii.gz first, then .nii
            image_path = os.path.join(image_dir, pid + ".nii.gz")
            if not os.path.exists(image_path):
                image_path = os.path.join(image_dir, pid + ".nii")

            label_path = os.path.join(label_dir, pid + ".nii.gz")
            if not os.path.exists(label_path):
                label_path = os.path.join(label_dir, pid + ".nii")

            vol = nib.load(image_path).get_fdata()
            seg = nib.load(label_path).get_fdata()

            # Downsample / upsample to fixed cube
            vol = resize(vol, shape)
            seg = resize(seg, shape, order=0, preserve_range=True).astype(np.float32)
            seg = (seg > 0.5).astype(np.float32)  # binarize mask

            masked_vol = vol * seg             # zero out non-liver voxels
            gather_samples.append(masked_vol.reshape(-1, 1))

            progress.update(task_id, advance=1)

    big_array = np.concatenate(gather_samples, axis=0)
    scaler = StandardScaler()
    scaler.fit(big_array)

    cache_dict = {}  # pid ➔ (3, 64, 64, 64) NumPy tensor

    def build_data(pid_list, desc):
        """
        Second pass: apply StandardScaler, duplicate channel-wise to 3,
        and package dict entries for each split.
        """
        items = []
        with Progress(
            TextColumn("{task.description}", style="black", justify="center"),
            BarColumn(bar_width=None, complete_style="green", finished_style="green"),
            TaskProgressColumn(),
            TextColumn("{task.completed}/{task.total} done", style="black", justify="center"),
            console=Console(),
            transient=True
        ) as p2:
            t_id = p2.add_task(desc, total=len(pid_list))
            for pid_ in pid_list:
                row_ = clinical_df[clinical_df["Image_Label_Base"] == pid_]
                if len(row_) == 0:
                    p2.update(t_id, advance=1)
                    continue

                image_path_ = os.path.join(image_dir, pid_ + ".nii.gz")
                if not os.path.exists(image_path_):
                    image_path_ = os.path.join(image_dir, pid_ + ".nii")

                label_path_ = os.path.join(label_dir, pid_ + ".nii.gz")
                if not os.path.exists(label_path_):
                    label_path_ = os.path.join(label_dir, pid_ + ".nii")

                vol_ = nib.load(image_path_).get_fdata()
                seg_ = nib.load(label_path_).get_fdata()
                vol_ = resize(vol_, shape)
                seg_ = resize(seg_, shape, order=0, preserve_range=True).astype(np.float32)
                seg_ = (seg_ > 0.5).astype(np.float32)
                masked_vol_ = vol_ * seg_

                # z-score with cohort statistics
                masked_vol_ = masked_vol_.reshape(-1, 1)
                masked_vol_ = scaler.transform(masked_vol_)
                masked_vol_ = masked_vol_.reshape(*shape)

                # Repeat to 3-channels (pretrained 3-D convs often expect ≥2 channels)
                vol_3c = np.repeat(masked_vol_[np.newaxis, ...], 3, axis=0)

                st_ = row_["Surv Time Censor 4"].values[0]
                ev_ = row_["Censor 4: HDFS"].values[0]

                cache_dict[pid_] = vol_3c.astype(np.float32)
                items.append({
                    'pid':        pid_,
                    'cache_pid':  pid_,
                    'image':      None,   # placeholder for future disk-based loading
                    'time':       float(st_),
                    'event':      bool(ev_)
                })
                p2.update(t_id, advance=1)
        return items

    train_data = build_data(train_list, "Processing and scaling (train)")
    val_data   = build_data(val_list,   "Processing and scaling (val)")

    final_dict = {
        'train': train_data,
        'val':   val_data,
        'mean':  scaler.mean_[0],
        'std':   scaler.scale_[0],
        'cache_dict': cache_dict
    }
    np.save(npy_save_path, final_dict)
    console.print(f"Saved data to {npy_save_path}")

##############################################################################
# PyTorch Dataset for Liver ROI
##############################################################################
class LiverDataset(torch.utils.data.Dataset):
    """
    Dataset wrapper that:
      • Returns 3-channel liver volume tensors
      • Optionally performs heavy 3-D augmentations on-the-fly
      • Provides survival time + event flag for DeepSurv loss
    """
    def __init__(self, data_list, cache_dict,
                 augment: bool = False,
                 tensor_cache: dict | None = None):
        self.data_list   = data_list
        self.cache_dict  = cache_dict
        self.augment     = augment
        self.tensor_cache = tensor_cache  # speed-up for val/test

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        d   = self.data_list[idx]
        pid = d['cache_pid']

        # If not augmenting and we already have CPU tensors cached, use them
        if (not self.augment) and (self.tensor_cache is not None):
            vol_3c = self.tensor_cache[pid].clone()
        else:
            vol_3c = self.cache_dict[pid].copy()

        # Heavy 3-D augmentations (intensity + spatial)
        if self.augment:
            vol_3c_t = np.transpose(vol_3c, (1, 2, 3, 0))  # (H, W, D, C)
            vol_3c_t = augment_transform(vol_3c_t)
            vol_3c   = np.transpose(vol_3c_t, (3, 0, 1, 2))

        return {
            'Whole_Liver_image': torch.tensor(vol_3c, dtype=torch.float32),
            'survival_time':     torch.tensor(d['time'],  dtype=torch.float32),
            'event_occurred':    torch.tensor(d['event'], dtype=torch.bool),
            'pid':               d['pid']
        }

##############################################################################
# Model components
##############################################################################
class Swish(nn.Module):
    """Memory-efficient SiLU/Swish activation."""
    def forward(self, x):
        return x * torch.sigmoid(x)

# Map string → activation
ACTIVATIONS = {
    'ReLU':      nn.ReLU,
    'ELU':       nn.ELU,
    'LeakyReLU': nn.LeakyReLU,
    'Swish':     Swish
}

def init_weights(m):
    """Xavier normal for conv/linear layers."""
    if isinstance(m, (nn.Linear, nn.Conv3d)):
        nn.init.xavier_normal_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)

# ---------------- Channel & spatial attention blocks (CBAM-3D) ----------------
class ChannelAttention3D(nn.Module):
    def __init__(self, in_planes, reduction=16):
        super().__init__()
        self.avgp = nn.AdaptiveAvgPool3d(1)
        self.maxp = nn.AdaptiveMaxPool3d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_planes, in_planes // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_planes // reduction, in_planes, bias=False)
        )
    def forward(self, x):
        b, c, _, _, _ = x.size()
        avg_out = self.fc(self.avgp(x).view(b, c))
        max_out = self.fc(self.maxp(x).view(b, c))
        out = torch.sigmoid(avg_out + max_out).view(b, c, 1, 1, 1)
        return x * out

class SpatialAttention3D(nn.Module):
    def __init__(self, kernel_size=7):
        super().__init__()
        pad = (kernel_size - 1) // 2
        self.conv = nn.Conv3d(2, 1, kernel_size=kernel_size,
                              padding=pad, bias=False)
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        out = torch.cat([avg_out, max_out], dim=1)
        out = torch.sigmoid(self.conv(out))
        return x * out

class CBAM3D(nn.Module):
    """Convolutional Block Attention Module (3-D variant)."""
    def __init__(self, channels, reduction=16, spatial_kernel_size=7):
        super().__init__()
        self.ca = ChannelAttention3D(channels, reduction)
        self.sa = SpatialAttention3D(spatial_kernel_size)
    def forward(self, x):
        x = self.ca(x)
        x = self.sa(x)
        return x

# ---------------- Basic residual block used in 3-D CNN backbone --------------
class ResidualBlock3D(nn.Module):
    def __init__(self, in_channels, out_channels,
                 stride=1, activation='Swish', norm=True):
        super().__init__()
        act_fn   = ACTIVATIONS[activation]()
        self.conv1 = nn.Conv3d(in_channels, out_channels, 3,
                               stride=stride, padding=1)
        self.bn1   = nn.BatchNorm3d(out_channels) if norm else nn.Identity()
        self.act   = act_fn
        self.conv2 = nn.Conv3d(out_channels, out_channels, 3,
                               padding=1)
        self.bn2   = nn.BatchNorm3d(out_channels) if norm else nn.Identity()

        self.shortcut = nn.Identity()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv3d(in_channels, out_channels, 1, stride=stride),
                nn.BatchNorm3d(out_channels) if norm else nn.Identity()
            )
    def forward(self, x):
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.act(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out += self.shortcut(x)
        out = self.act(out)
        return out

# ---------------- Full 3-D CNN feature extractor -----------------------------
class Advanced3DCNN(nn.Module):
    """
    Lightweight yet expressive backbone:
      • 4 stages with residual blocks & CBAM
      • Global-average-pooled output → DeepSurv fully-connected head
    """
    def __init__(self, activation='Swish', norm=True, drop=0.2):
        super().__init__()
        self.layer0 = nn.Sequential(
            nn.Conv3d(3, 16, 3, padding=1),
            nn.BatchNorm3d(16) if norm else nn.Identity(),
            ACTIVATIONS[activation](),
            nn.MaxPool3d(2)
        )
        self.res1  = ResidualBlock3D(16,  32, 2, activation, norm)
        self.res2  = ResidualBlock3D(32,  64, 2, activation, norm)
        self.res3  = ResidualBlock3D(64, 128, 2, activation, norm)
        self.cbam  = CBAM3D(128, 16, 7)
        self.gap   = nn.AdaptiveAvgPool3d(1)
        self.drop  = nn.Dropout3d(drop)

        self.apply(init_weights)

    def forward(self, x):
        x = self.layer0(x)
        x = self.res1(x)
        x = self.res2(x)
        x = self.res3(x)
        x = self.cbam(x)
        x = self.gap(x)
        x = self.drop(x)
        return torch.flatten(x, 1)

# ---------------- DeepSurv head ---------------------------------------------
class DeepSurv(nn.Module):
    """
    Fully-connected Cox proportional hazards model with
    (optional) dropout and batch-norm between hidden layers.
    """
    def __init__(self, dims, drop=0.2,
                 norm=True, activation='Swish'):
        super().__init__()
        act_fn = ACTIVATIONS[activation]
        layers = []
        for i in range(len(dims) - 1):
            if i > 0 and drop is not None:
                layers.append(nn.Dropout(drop))
            layers.append(nn.Linear(dims[i], dims[i + 1]))
            if norm and i < len(dims) - 1:
                layers.append(nn.BatchNorm1d(dims[i + 1]))
            if i < len(dims) - 1:
                layers.append(act_fn())
        self.model = nn.Sequential(*layers)
        self.apply(init_weights)

    def forward(self, X):
        return self.model(X)

##############################################################################
# EMA utility (exponential moving average of model weights)
##############################################################################
class EMA:
    """
    Maintain a shadow copy of model weights for
    smoother validation curves and better generalization.
    """
    def __init__(self, model, decay=0.999):
        self.model  = model
        self.decay  = decay
        self.shadow = {}
        self.shadow_buffers = {}
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
        for name, buf in model.named_buffers():
            self.shadow_buffers[name] = buf.clone()

    def update(self):
        with torch.no_grad():
            for name, param in self.model.named_parameters():
                if param.requires_grad:
                    self.shadow[name] = (
                        self.decay * self.shadow[name]
                        + (1 - self.decay) * param.data
                    )
            for name, buf in self.model.named_buffers():
                self.shadow_buffers[name] = buf.clone()

    def apply_shadow(self):
        """Swap live weights with EMA weights in-place."""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data.copy_(self.shadow[name])
        for name, buf in self.model.named_buffers():
            buf.copy_(self.shadow_buffers[name])

def get_full_ema_state(ema):
    """
    Return dict that can be loaded directly into a fresh model
    (merges parameters and buffers in a single state-dict).
    """
    full_state   = {}
    model_state  = ema.model.state_dict()
    for name in model_state.keys():
        if name in ema.shadow:
            full_state[name] = ema.shadow[name]
        elif name in ema.shadow_buffers:
            full_state[name] = ema.shadow_buffers[name]
        else:
            full_state[name] = model_state[name]
    return full_state

##############################################################################
# Test-time augmentation + Monte-Carlo dropout
##############################################################################
def enable_mc_dropout(model):
    """Force dropout / deactivate running-BN stats during MC sampling."""
    for m in model.modules():
        if m.__class__.__name__.startswith('Dropout'):
            m.train()
        elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            m.eval()

def apply_tta_mc_dropout(feature_extractor, deep_surv, batch,
                         num_tta=5, num_mc=5):
    """
    • *num_tta*  — how many geometric TTA passes (here just identity,
                   but hook point is provided).  
    • *num_mc*   — MC-dropout samples per TTA pass.  
    Returns averaged risk predictions.
    """
    set_eval_seed(EVAL_SEED)
    feature_extractor.eval()
    deep_surv.eval()
    enable_mc_dropout(feature_extractor)
    enable_mc_dropout(deep_surv)

    base_feats = batch['Whole_Liver_image'].cpu().numpy()
    pred_list  = []
    for _ in range(num_tta):
        # ➜ plug-in spatial augmentations here if desired
        aug_tensor = torch.tensor(np.stack(base_feats, axis=0),
                                  dtype=torch.float32,
                                  device=device)
        mc_preds = []
        for _mc in range(num_mc):
            with autocast(enabled=(device.type == 'cuda')):
                emb_ = feature_extractor(aug_tensor)
                out_ = deep_surv(emb_).detach().cpu().numpy().ravel()
            mc_preds.append(out_)
        pred_list.append(np.mean(mc_preds, axis=0))

    final_preds = np.mean(pred_list, axis=0)
    return final_preds

def evaluate_model(feature_extractor, deep_surv,
                   dataset, batch_size):
    """Evaluate on entire *dataset* with TTA+MC-Dropout."""
    set_eval_seed(EVAL_SEED)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False,
                        num_workers=2, pin_memory=(device.type == 'cuda'))
    all_t, all_e, all_p = [], [], []
    for batch in loader:
        t_ = batch['survival_time'].cpu().numpy()
        e_ = batch['event_occurred'].cpu().numpy().astype(bool)
        preds_ = apply_tta_mc_dropout(feature_extractor, deep_surv, batch,
                                      num_tta=5, num_mc=5)
        all_t.append(t_)
        all_e.append(e_)
        all_p.append(preds_)
    return (np.concatenate(all_t),
            np.concatenate(all_e),
            np.concatenate(all_p))

def compute_uno_cindex(train_data, val_data):
    """
    Uno's c-index — de-biased for right-censoring.
    train_data / val_data are tuples (t, e, p).
    """
    s_tr  = Surv.from_arrays(event=train_data[1], time=train_data[0])
    s_val = Surv.from_arrays(event=val_data[1], time=val_data[0])
    return concordance_index_ipcw(s_tr, s_val, val_data[2])[0]

##############################################################################
# Loss & L2-regularization wrapper
##############################################################################
class Regularization(nn.Module):
    """Generic L_p weight decay (defaults to L2)."""
    def __init__(self, order=2, weight_decay=1e-4):
        super().__init__()
        self.order = order
        self.weight_decay = weight_decay
    def forward(self, model):
        reg_val = 0
        for name, w in model.named_parameters():
            if w.requires_grad and 'weight' in name:
                reg_val += torch.norm(w, p=self.order)
        return self.weight_decay * reg_val

class CoxPartialLogLikelihood(nn.Module):
    """
    Negative partial log-likelihood for Cox models,
    plus optional L2 penalty on network weights.
    """
    def __init__(self, config):
        super().__init__()
        self.L2_reg = config['l2_reg']
        self.reg    = Regularization(order=2, weight_decay=self.L2_reg)
    def forward(self, risk_pred, y, e, model):
        sort_idx = torch.argsort(y)          # ascending survival times
        srisk    = risk_pred[sort_idx]
        se       = e[sort_idx]
        exprisk  = torch.exp(srisk)
        cs       = torch.flip(torch.cumsum(torch.flip(exprisk, [0]), dim=0), [0])
        lse      = torch.log(cs)
        part_ll  = torch.sum(se * (srisk - lse))
        neg_log  = -part_ll / (torch.sum(e) + 1e-8)
        return neg_log + self.reg(model)

##############################################################################
# (Optional) self-supervised learning heads
##############################################################################
"""
RotationHead and early SSL code retained for reference,
but 8-class transform SSL (below) is the recommended option.
"""

##############################################################################
# SSL head for 8-class 3-D transform prediction
##############################################################################
class MultiTransformHead(nn.Module):
    def __init__(self, in_dim):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(in_dim, in_dim // 2),
            nn.ReLU(),
            nn.Linear(in_dim // 2, 8)   # eight discrete transforms
        )
        init_weights(self.fc[0])
        init_weights(self.fc[2])
    def forward(self, x):
        return self.fc(x)

def apply_3d_transform_8class(volume, transform_id):
    """
    Enumerated 3-D transforms:
       0–3 = 0°, 90°, 180°, 270° rotations in axial plane
       4–7 = same rotations but with x-flip
    """
    rot_id  = transform_id % 4
    flip_id = transform_id // 4

    rotated = volume
    if rot_id > 0:
        rotated = np.rot90(rotated, k=rot_id, axes=(2, 3)).copy()
    if flip_id == 1:
        rotated = np.flip(rotated, axis=1).copy()
    return rotated

##############################################################################
# Embedding caching helpers (for hyper-param search)
##############################################################################
class CachedEmbeddingDataset(torch.utils.data.Dataset):
    """Lightweight wrapper where each item is already a feature vector."""
    def __init__(self, items):
        self.items = items
    def __len__(self):
        return len(self.items)
    def __getitem__(self, idx):
        return self.items[idx]

def compute_embeddings_for_dataset(feature_extractor, dataset):
    """
    Pass the entire dataset once; cache flattened embeddings
    to avoid recomputing them during hyper-parameter search.
    """
    feature_extractor.eval()
    loader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=2)
    emb_items = []
    with torch.no_grad():
        for batch in loader:
            vol_ = batch['Whole_Liver_image'].to(device)
            t_   = batch['survival_time']
            e_   = batch['event_occurred']
            pid_ = batch['pid']
            emb_ = feature_extractor(vol_)
            for i in range(vol_.size(0)):
                emb_items.append({
                    'embedding': emb_[i].cpu(),
                    'time':      t_[i].item(),
                    'event':     e_[i].item(),
                    'pid':       pid_[i]
                })
    return emb_items

# ---------------- Training helper that works purely on cached embeddings -----
def train_val_on_embeddings(emb_dataset, ds, train_idx, cfg):
    """
    Fast inner-loop training during random-search HPO.
    Operates on cached `emb_dataset` instead of raw voxels.
    """
    from torch.optim.lr_scheduler import OneCycleLR
    loss_fn = CoxPartialLogLikelihood(cfg)
    opt     = torch.optim.Adam(ds.parameters(),
                               lr=cfg['lr'],
                               weight_decay=cfg['weight_decay'])
    sc      = GradScaler(enabled=(device.type == 'cuda'))

    sampler  = SubsetRandomSampler(train_idx)
    loader   = DataLoader(emb_dataset, batch_size=cfg['batch_size'],
                          sampler=sampler, num_workers=2, drop_last=True)
    steps_pe = len(loader)
    max_ep   = 2         # keep inner loop short
    sched    = OneCycleLR(opt, max_lr=cfg['lr'],
                          steps_per_epoch=steps_pe, epochs=max_ep)

    ds.train()
    for _ in range(max_ep):
        for batch in loader:
            emb = batch['embedding'].to(device)
            t   = torch.tensor(batch['time'],  device=device)
            e   = torch.tensor(batch['event'], device=device, dtype=torch.bool)

            opt.zero_grad()
            with autocast(enabled=(device.type == 'cuda')):
                out  = ds(emb)
                loss = loss_fn(out, t, e, ds)
            sc.scale(loss).backward()
            sc.step(opt)
            sc.update()
            sched.step()

    # Evaluate c-index on full embedding dataset
    ds.eval()
    loader_eval = DataLoader(emb_dataset, batch_size=cfg['batch_size'],
                             shuffle=False)
    all_t, all_e, all_p = [], [], []
    with torch.no_grad():
        for b in loader_eval:
            e   = b['embedding'].to(device)
            t_v = np.array(b['time'])
            e_v = np.array(b['event']).astype(bool)
            o   = ds(e).cpu().numpy().squeeze()
            all_t.append(t_v)
            all_e.append(e_v)
            all_p.append(o)
    all_t = np.concatenate(all_t)
    all_e = np.concatenate(all_e)
    all_p = np.concatenate(all_p)

    s_tr  = Surv.from_arrays(event=all_e, time=all_t)
    return concordance_index_ipcw(s_tr, s_tr, all_p)[0]

def hyperparam_search_with_embeddings(emb_dataset, k=3):
    """
    Random-grid search over a handful of hyper-parameters,
    evaluated via k-fold CV on cached embeddings.
    """
    from itertools import product
    lr_list       = [1e-4, 3e-4, 5e-4, 1e-3]
    drop_list     = [0.1, 0.2, 0.3]
    hidden_dim_list = [128, 256]
    act_list      = ['Swish', 'LeakyReLU']
    bsz_list      = [4, 8, 16]
    wdecay        = [1e-5, 1e-4]
    l2reg         = [1e-5, 1e-4]

    indices = list(range(len(emb_dataset)))
    random.shuffle(indices)
    kf = KFold(n_splits=k, shuffle=True, random_state=42)

    best_c, best_cfg = -999, None
    all_comb = list(product(lr_list, drop_list, hidden_dim_list,
                            act_list, bsz_list, wdecay, l2reg))

    with Progress(
        TextColumn("{task.description}", style="black", justify="center"),
        BarColumn(bar_width=None, complete_style="green", finished_style="green"),
        TaskProgressColumn(),
        TextColumn("{task.completed}/{task.total} done", style="black", justify="center"),
        transient=True,
        console=console
    ) as progress:
        search_task = progress.add_task("Hyperparameter Search", total=len(all_comb))
        for lr_, dp_, hd_, act_, bsz_, wd_, l2_ in all_comb:
            cfg_ = {
                'lr':           lr_,
                'drop':         dp_,
                'hidden_dim':   hd_,
                'activation':   act_,
                'batch_size':   bsz_,
                'weight_decay': wd_,
                'l2_reg':       l2_,
            }
            fold_scores = []
            for tri, _ in kf.split(indices):
                ds_ = DeepSurv([emb_dataset[0]['embedding'].shape[0],
                                hd_, hd_, 1],
                               drop=dp_, activation=act_, norm=True).to(device)
                c_ = train_val_on_embeddings(emb_dataset, ds_, tri, cfg_)
                fold_scores.append(c_)
            mean_ = np.mean(fold_scores)
            if mean_ > best_c:
                best_c  = mean_
                best_cfg = cfg_
            progress.update(search_task, advance=1)

    return best_cfg, best_c

##############################################################################
# Stratified sampler (balance events/non-events each epoch)
##############################################################################
def create_stratified_sampler(dset):
    e_list = [dset[i]['event_occurred'].item() for i in range(len(dset))]
    e_list = np.array(e_list)
    ev_ct  = (e_list == 1).sum()
    cn_ct  = (e_list == 0).sum()
    weights           = np.zeros_like(e_list, dtype=float)
    weights[e_list==1] = 2.0 / ev_ct
    weights[e_list==0] = 1.0 / cn_ct
    return WeightedRandomSampler(weights, len(weights), replacement=True)

##############################################################################
# SSL pre-training (8-class transform) — optional but useful
##############################################################################
def train_ssl(fe, dataset, cfg, seed):
    """
    Lightweight self-supervised pre-training on liver volumes
    to improve representation quality before survival training.
    Saves feature extractor weights to `ssl_best_fe_seed{seed}.pt`.
    """
    ssl_weights_path = os.path.join(results_dir,
                                    f'ssl_best_fe_seed{seed}.pt')
    if os.path.exists(ssl_weights_path):
        console.print(f"Seed {seed}: Found SSL weights, skipping SSL training.")
        fe.load_state_dict(torch.load(ssl_weights_path,
                                      map_location=device))
        return

    # Re-wrap dataset ➔ plain volume list for SSL
    class SSLDataset(torch.utils.data.Dataset):
        def __init__(self, data_list):
            self.data_list = data_list
        def __len__(self):
            return len(self.data_list)
        def __getitem__(self, idx):
            dd = self.data_list[idx]
            return torch.tensor(dd['Whole_Liver_image'],
                                dtype=torch.float32)

    data_list_equiv = [
        {'Whole_Liver_image': dataset[i]['Whole_Liver_image'].cpu().numpy()}
        for i in range(len(dataset))
    ]
    ssl_ds = SSLDataset(data_list_equiv)

    # 90–10 split inside SSL set
    idx_ssl = list(range(len(ssl_ds)))
    random.shuffle(idx_ssl)
    val_sp        = int(0.1 * len(idx_ssl))
    ssl_val_idx   = idx_ssl[:val_sp]
    ssl_tr_idx    = idx_ssl[val_sp:]

    ssl_tr_loader = DataLoader(ssl_ds, batch_size=cfg['batch_size'],
                               sampler=SubsetRandomSampler(ssl_tr_idx),
                               num_workers=2, drop_last=True)
    ssl_val_loader= DataLoader(ssl_ds, batch_size=cfg['batch_size'],
                               sampler=SubsetRandomSampler(ssl_val_idx),
                               num_workers=2)

    with torch.no_grad():
        dummy = torch.zeros(1, 3, 64, 64, 64).to(device)
        conv_dim = fe(dummy).shape[1]

    ssl_head = MultiTransformHead(conv_dim).to(device)
    ssl_opt  = torch.optim.Adam(list(fe.parameters()) +
                                list(ssl_head.parameters()),
                                lr=cfg['lr'], weight_decay=cfg['weight_decay'])
    ssl_ce   = nn.CrossEntropyLoss()
    from torch.optim.lr_scheduler import OneCycleLR

    steps_pe  = len(ssl_tr_loader)
    ssl_max_e = 100
    ssl_pat   = 20
    ssl_sched = OneCycleLR(ssl_opt, max_lr=cfg['lr'],
                           steps_per_epoch=steps_pe,
                           epochs=ssl_max_e)
    sc = GradScaler(enabled=(device.type == 'cuda'))

    def run_ssl_epoch(loader, train_mode=True):
        fe.train() if train_mode else fe.eval()
        ssl_head.train() if train_mode else ssl_head.eval()

        total_loss, total_count = 0, 0
        for imgs in loader:
            imgs = imgs.to(device)
            transform_id = random.randint(0, 7)

            # Apply 8-class transform to each sample independently
            bsz = imgs.size(0)
            vols = []
            for i in range(bsz):
                vol_np = imgs[i].cpu().numpy()
                vols.append(
                    apply_3d_transform_8class(vol_np, transform_id)[np.newaxis, ...]
                )
            vols = torch.tensor(np.concatenate(vols, axis=0),
                                dtype=torch.float32, device=device)

            ssl_opt.zero_grad()
            with autocast(enabled=(device.type == 'cuda')):
                emb = fe(vols)
                log = ssl_head(emb)
                tgt = torch.tensor([transform_id] * bsz,
                                   dtype=torch.long, device=device)
                loss = ssl_ce(log, tgt)
            if train_mode:
                sc.scale(loss).backward()
                sc.step(ssl_opt)
                sc.update()
                ssl_sched.step()

            total_loss  += loss.item() * bsz
            total_count += bsz

        return total_loss / total_count if total_count > 0 else 0

    best_ssl_val, no_imp = 9999, 0
    with Progress(console=console, transient=True) as ssl_progress:
        ssl_task = ssl_progress.add_task("SSL Epochs", total=ssl_max_e)
        for ep in range(1, ssl_max_e + 1):
            _ = run_ssl_epoch(ssl_tr_loader, train_mode=True)
            v_l = run_ssl_epoch(ssl_val_loader, train_mode=False)
            if v_l < best_ssl_val:
                best_ssl_val = v_l
                no_imp = 0
                torch.save(fe.state_dict(), ssl_weights_path)
            else:
                no_imp += 1
                if no_imp >= ssl_pat:
                    ssl_progress.update(ssl_task, advance=1)
                    break
            ssl_progress.update(ssl_task, advance=1)

    fe.load_state_dict(torch.load(ssl_weights_path,
                                  map_location=device))

##############################################################################
# End-to-end training routine for a single random seed
##############################################################################
def train_single_seed(seed, train_dataset, val_dataset,
                      cfg, do_ssl=True):
    console.print(f"Training with seed={seed}...")
    set_seed(seed)

    # Instantiate backbone + head
    fe = Advanced3DCNN(activation=cfg['activation'],
                       drop=cfg['drop'], norm=True).to(device)
    with torch.no_grad():
        dummy = torch.zeros(1, 3, 64, 64, 64).to(device)
        conv_out_dim = fe(dummy).shape[1]

    ds = DeepSurv([conv_out_dim,
                   cfg['hidden_dim'],
                   cfg['hidden_dim'],
                   1],
                  drop=cfg['drop'],
                  activation=cfg['activation'],
                  norm=True).to(device)

    # Optional SSL warm-up
    if do_ssl:
        train_ssl(fe, train_dataset, cfg, seed)

    loss_fn_s = CoxPartialLogLikelihood(cfg)
    params    = list(fe.parameters()) + list(ds.parameters())
    opt_s     = torch.optim.Adam(params, lr=cfg['lr'],
                                 weight_decay=cfg['weight_decay'])

    # Stratified sampling to balance censor/event
    train_sampler = create_stratified_sampler(train_dataset)
    train_loader  = DataLoader(train_dataset, batch_size=cfg['batch_size'],
                               sampler=train_sampler, num_workers=2, drop_last=True)
    val_loader    = DataLoader(val_dataset, batch_size=cfg['batch_size'],
                               shuffle=False, num_workers=2)

    from torch.optim.lr_scheduler import OneCycleLR
    steps_pe2 = len(train_loader)
    max_ep2   = cfg.get('max_epoch', 200)
    early_st  = cfg.get('early_stop', 30)
    sched_s   = OneCycleLR(opt_s, max_lr=cfg['lr'],
                           steps_per_epoch=steps_pe2, epochs=max_ep2)

    ema_fe = EMA(fe, 0.999)
    ema_ds = EMA(ds, 0.999)

    best_val, stop_ct, best_ckpt = -999, 0, None

    with Progress(
        TextColumn("{task.description}", style="black", justify="center"),
        BarColumn(bar_width=None, complete_style="green", finished_style="green"),
        TaskProgressColumn(),
        TextColumn("{task.completed}/{task.total} done", style="black", justify="center"),
        console=Console(),
        transient=True
    ) as epoch_progress:
        epoch_task_id = epoch_progress.add_task("Epochs", total=max_ep2)

        for ep in range(1, max_ep2 + 1):
            fe.train()
            ds.train()
            for batch in train_loader:
                feat = batch['Whole_Liver_image'].to(device)
                t    = batch['survival_time'].to(device)
                e    = batch['event_occurred'].to(device)

                opt_s.zero_grad()
                with autocast(enabled=(device.type == 'cuda')):
                    out = ds(fe(feat))
                    ls  = loss_fn_s(out, t, e, ds)
                scaler_amp.scale(ls).backward()
                scaler_amp.step(opt_s)
                scaler_amp.update()
                sched_s.step()

                ema_fe.update()
                ema_ds.update()

            # ---------- EMA validation ----------
            fe_backup, ds_backup = deepcopy(fe.state_dict()), deepcopy(ds.state_dict())

            ema_fe.apply_shadow()
            ema_ds.apply_shadow()

            train_dat = evaluate_model(fe, ds, train_dataset, cfg['batch_size'])
            val_dat   = evaluate_model(fe, ds, val_dataset,   cfg['batch_size'])
            cidx      = compute_uno_cindex(train_dat, val_dat)

            # Revert weights
            fe.load_state_dict(fe_backup)
            ds.load_state_dict(ds_backup)

            if cidx > best_val:
                best_val = cidx
                stop_ct  = 0
                ema_fe_state = get_full_ema_state(ema_fe)
                ema_ds_state = get_full_ema_state(ema_ds)
                best_ckpt = {
                    'feature_extractor': ema_fe_state,
                    'deep_surv':         ema_ds_state,
                    'val_cindex':        cidx,
                    'epoch':             ep
                }
                console.print(f"Epoch {ep}: Best c-index improved to {cidx:.4f}")
            else:
                stop_ct += 1
                if stop_ct >= early_st:
                    console.print(f"Early stopping triggered at epoch {ep}")
                    break

            epoch_progress.update(epoch_task_id, advance=1)

    # Save best checkpoint for this seed
    final_ckpt_path = os.path.join(results_dir,
                                   f'final_best_model_seed{seed}.pt')
    if best_ckpt is not None:
        torch.save(best_ckpt, final_ckpt_path)
        console.print(f"Saved best checkpoint (c-index={best_val:.4f}) ➜ {final_ckpt_path}")
    else:
        console.print("No best checkpoint saved.")
        return -1

    # Quick sanity reload
    re_ckpt = torch.load(final_ckpt_path, map_location=device)

    re_fe = Advanced3DCNN(activation=cfg['activation'],
                          drop=cfg['drop']).to(device)
    with torch.no_grad():
        tmp  = torch.zeros(1, 3, 64, 64, 64).to(device)
        conv_dim_ = re_fe(tmp).shape[1]
    re_ds = DeepSurv([conv_dim_,
                      cfg['hidden_dim'],
                      cfg['hidden_dim'],
                      1],
                     drop=cfg['drop'],
                     activation=cfg['activation']).to(device)

    re_fe.load_state_dict(re_ckpt['feature_extractor'], strict=False)
    re_ds.load_state_dict(re_ckpt['deep_surv'],         strict=False)
    re_fe.eval()
    re_ds.eval()

    tr_dat = evaluate_model(re_fe, re_ds, train_dataset, cfg['batch_size'])
    val_dat= evaluate_model(re_fe, re_ds, val_dataset,   cfg['batch_size'])
    reload_cidx = compute_uno_cindex(tr_dat, val_dat)

    console.print(f"Reloaded model c-index (val) = {reload_cidx:.4f}")
    return reload_cidx

##############################################################################
# Lightweight helpers for loading saved models & ensembles
##############################################################################
def load_trained_model(seed_, cfg):
    fe_ = Advanced3DCNN(activation=cfg['activation'],
                        drop=cfg['drop']).to(device)
    with torch.no_grad():
        tmp = torch.zeros(1, 3, 64, 64, 64).to(device)
        conv_dim_ = fe_(tmp).shape[1]
    ds_ = DeepSurv([conv_dim_,
                    cfg['hidden_dim'],
                    cfg['hidden_dim'],
                    1],
                   drop=cfg['drop'],
                   activation=cfg['activation']).to(device)
    cpath = os.path.join(results_dir,
                         f'final_best_model_seed{seed_}.pt')
    ck_   = torch.load(cpath, map_location=device)
    fe_.load_state_dict(ck_['feature_extractor'])
    ds_.load_state_dict(ck_['deep_surv'])
    fe_.eval()
    ds_.eval()
    return fe_, ds_

def evaluate_ensemble(dataset, models, cfg, aggregator='mean'):
    """
    Aggregate predictions from multiple (feature_extractor, deep_surv)
    tuples. Choose among 'mean' / 'median' / 'max'.
    """
    set_eval_seed(EVAL_SEED)
    loader = DataLoader(dataset, batch_size=cfg['batch_size'],
                        shuffle=False, num_workers=2)
    all_t, all_e, all_p = [], [], []
    for batch in loader:
        t_ = batch['survival_time'].cpu().numpy()
        e_ = batch['event_occurred'].cpu().numpy().astype(bool)
        sub_preds = []
        for (fe_, ds_) in models:
            p_ = apply_tta_mc_dropout(fe_, ds_, batch,
                                      num_tta=5, num_mc=5)
            sub_preds.append(p_)
        sub_preds = np.array(sub_preds)
        if aggregator == 'median':
            final_pred = np.median(sub_preds, axis=0)
        elif aggregator == 'max':
            final_pred = np.max(sub_preds, axis=0)
        else:
            final_pred = np.mean(sub_preds, axis=0)
        all_t.append(t_)
        all_e.append(e_)
        all_p.append(final_pred)
    return (np.concatenate(all_t),
            np.concatenate(all_e),
            np.concatenate(all_p))

##############################################################################
# Main orchestration logic
##############################################################################
def main():
    console.print("✅  Liver HDFS pipeline — sanity-check your paths then run.")

    import torch.backends.cudnn as cudnn
    cudnn.benchmark = True  # speed up fixed-size 3-D kernels

    # ----------------------------------------------------------------------
    # 1) Prepare or load cached NPY dataset
    # ----------------------------------------------------------------------
    npy_path = os.path.join(cached_dir, "Final_HDFS_data.npy")
    if not os.path.exists(npy_path):
        create_np_data(liver_image_dir, liver_label_dir,
                       merged_data,
                       train_image_names, val_image_names,
                       npy_save_path=npy_path,
                       shape=(64, 64, 64))
    else:
        console.print("Found existing NPY cache — skipping re-creation.")

    dat_all   = np.load(npy_path, allow_pickle=True).item()
    train_list = dat_all['train']
    val_list   = dat_all['val']
    cache_dict = dat_all['cache_dict']

    # CPU tensors to accelerate evaluation
    val_cache_dict_tensors = {
        item['cache_pid']: torch.from_numpy(cache_dict[item['cache_pid']]).float()
        for item in val_list
    }
    train_cache_dict_tensors = {
        item['cache_pid']: torch.from_numpy(cache_dict[item['cache_pid']]).float()
        for item in train_list
    }

    train_dataset = LiverDataset(train_list, cache_dict,
                                 augment=True,
                                 tensor_cache=train_cache_dict_tensors)
    val_dataset   = LiverDataset(val_list,   cache_dict,
                                 augment=False,
                                 tensor_cache=val_cache_dict_tensors)

    # ----------------------------------------------------------------------
    # 2) Build (or load) toy embedding dataset for quick HPO
    # ----------------------------------------------------------------------
    embedding_data_path = os.path.join(cached_dir, "emb_dataset.pkl")
    if os.path.exists(embedding_data_path):
        with open(embedding_data_path, 'rb') as f:
            emb_dataset = pickle.load(f)
        console.print(f"Loaded cached embeddings from {embedding_data_path}.")
    else:
        console.print("No cached embeddings ➜ building simple mean-chunks.")
        emb_dataset = []
        for i in range(len(train_dataset)):
            ditem = train_dataset[i]
            vol   = ditem['Whole_Liver_image']  # (3, 64, 64, 64)
            flat  = vol.flatten()
            chunk_count = 128
            chunk_size  = flat.shape[0] // chunk_count or 1
            shortened   = flat[:chunk_size * chunk_count]
            chunk_means = torch.stack([s.mean()
                                       for s in torch.split(shortened,
                                                            chunk_size)])
            emb_dataset.append({
                'embedding': chunk_means,
                'time':      ditem['survival_time'].item(),
                'event':     ditem['event_occurred'].item(),
                'pid':       ditem['pid']
            })
        with open(embedding_data_path, 'wb') as f:
            pickle.dump(emb_dataset, f)
        console.print(f"Saved embeddings ➜ {embedding_data_path}")

    # ----------------------------------------------------------------------
    # 3) Hyper-parameter search (or load from disk)
    # ----------------------------------------------------------------------
    best_params_path = os.path.join(cached_dir, "best_params.pkl")
    if os.path.exists(best_params_path):
        with open(best_params_path, 'rb') as f:
            best_params = pickle.load(f)
        console.print("Loaded best hyper-parameters:")
        console.print(best_params)
    else:
        console.print("Running quick HPO on embeddings...")
        best_params, best_c = hyperparam_search_with_embeddings(emb_dataset, k=3)
        with open(best_params_path, 'wb') as f:
            pickle.dump(best_params, f)
        console.print("Best HPO params:")
        console.print(best_params)

    # ----------------------------------------------------------------------
    # 4) Train multiple seeds for an ensemble
    # ----------------------------------------------------------------------
    seeds       = [42, 1337, 2023, 9999, 888, 1010, 2022, 2222, 777, 555]
    seed_scores = []

    for sd in seeds:
        model_file = os.path.join(results_dir,
                                  f'final_best_model_seed{sd}.pt')
        if os.path.exists(model_file):
            console.print(f"Seed {sd} already trained — skipping.")
            fe_loaded, ds_loaded = load_trained_model(sd, best_params)
            tr_dat = evaluate_model(fe_loaded, ds_loaded,
                                    train_dataset, best_params['batch_size'])
            val_dat= evaluate_model(fe_loaded, ds_loaded,
                                    val_dataset, best_params['batch_size'])
            cidx   = compute_uno_cindex(tr_dat, val_dat)
            seed_scores.append((sd, cidx))
        else:
            cidx = train_single_seed(sd, train_dataset,
                                     val_dataset, best_params,
                                     do_ssl=True)
            seed_scores.append((sd, cidx))
        console.print(f"Seed {sd} ➜ best c-index {seed_scores[-1][1]:.4f}")

    # ----------------------------------------------------------------------
    # 5) Ensemble selection among top 5 seeds
    # ----------------------------------------------------------------------
    sorted_seeds_all = sorted(seed_scores,
                              key=lambda x: x[1], reverse=True)
    top5_seeds       = [sd for sd, _ in sorted_seeds_all[:5]]

    aggregations = ['mean', 'median', 'max']
    best_single_c       = -999
    best_ensemble_info  = None

    console.print(f"\nTesting ensemble combos across seeds {top5_seeds}\n")
    for i in range(1, len(top5_seeds) + 1):
        combo = top5_seeds[:i]
        for agg in aggregations:
            models_list = [load_trained_model(sd, best_params)
                           for sd in combo]
            train_ens = evaluate_ensemble(train_dataset, models_list,
                                          best_params, aggregator=agg)
            val_ens   = evaluate_ensemble(val_dataset,   models_list,
                                          best_params, aggregator=agg)
            single_c  = compute_uno_cindex(train_ens, val_ens)

            combo_str = ", ".join(str(s) for s in combo)
            console.print(f"[Seeds {combo_str} | {agg}] ➜ c-index={single_c:.4f}")

            if single_c > best_single_c:
                best_single_c = single_c
                best_ensemble_info = {
                    'subset':   tuple(combo),
                    'agg':      agg,
                    'single_c': single_c,
                    'models':   models_list
                }

    console.print("\n🏆 Best ensemble:")
    console.print(best_ensemble_info)

    # ----------------------------------------------------------------------
    # 6) Bootstrap CI for best ensemble
    # ----------------------------------------------------------------------
    models_list = best_ensemble_info['models']
    agg         = best_ensemble_info['agg']

    train_ens = evaluate_ensemble(train_dataset, models_list,
                                  best_params, aggregator=agg)
    val_ens   = evaluate_ensemble(val_dataset,   models_list,
                                  best_params, aggregator=agg)
    s_train   = Surv.from_arrays(event=train_ens[1],
                                 time=train_ens[0])

    num_boot   = 500
    cidx_list  = []

    with Progress(console=console, transient=True) as progress_boot:
        boot_task = progress_boot.add_task("Bootstrapping", total=num_boot)
        for _ in range(num_boot):
            idx = np.random.choice(len(val_ens[0]),
                                   size=len(val_ens[0]), replace=True)
            st, se, sp = val_ens[0][idx], val_ens[1][idx], val_ens[2][idx]
            s_val = Surv.from_arrays(event=se, time=st)
            c = concordance_index_ipcw(s_train, s_val, sp)[0]
            cidx_list.append(c)
            progress_boot.update(boot_task, advance=1)

    c_median = np.median(cidx_list)
    lo_, hi_ = np.percentile(cidx_list, [2.5, 97.5])

    console.print(f"\nBest ensemble single-run c-index = {best_single_c:.4f}")
    console.print(f"Bootstrap median = {c_median:.4f} (95 % CI: {lo_:.4f}–{hi_:.4f})")

    # ----------------------------------------------------------------------
    # 7) Export per-patient features + risk to CSV
    # ----------------------------------------------------------------------
    full_list   = train_list + val_list
    full_dataset = LiverDataset(full_list, cache_dict,
                                augment=False, tensor_cache=None)
    loader_all  = DataLoader(full_dataset, batch_size=1, shuffle=False)

    final_ids, final_feats, final_risks = [], [], []

    def get_model_prediction(fe_, ds_, vol):
        with torch.no_grad():
            emb = fe_(vol)
            out = ds_(emb)
        return emb.cpu().numpy().squeeze(), out.cpu().numpy().squeeze()

    for batch in loader_all:
        pid = batch['pid'][0]
        vol = batch['Whole_Liver_image'].to(device)

        sub_embs, sub_risks = [], []
        for (fe_, ds_) in models_list:
            emb_, risk_ = get_model_prediction(fe_, ds_, vol)
            sub_embs.append(emb_)
            sub_risks.append(risk_)

        emb_ens = np.mean(np.stack(sub_embs, axis=0), axis=0)

        sub_risks = np.array(sub_risks)
        if agg == 'median':
            risk_ens = np.median(sub_risks)
        elif agg == 'max':
            risk_ens = np.max(sub_risks)
        else:
            risk_ens = np.mean(sub_risks)

        final_ids.append(pid)
        final_feats.append(emb_ens)
        final_risks.append(risk_ens)

    final_feats = np.array(final_feats)
    final_risks = np.array(final_risks)

    feat_path = os.path.join(results_dir, "Liver_DeepFeatures_HDFS.csv")
    risk_path = os.path.join(results_dir, "Liver_RiskValues_HDFS.csv")

    # Write feature CSV
    with open(feat_path, 'w') as f:
        cols = ["patient_id"] + [f"feat_{i}" for i in range(final_feats.shape[1])]
        f.write(",".join(cols) + "\n")
        for pid, feats in zip(final_ids, final_feats):
            row = [pid] + [f"{v:.6f}" for v in feats]
            f.write(",".join(row) + "\n")

    # Write risk CSV
    with open(risk_path, 'w') as f:
        f.write("patient_id,risk\n")
        for pid, rk in zip(final_ids, final_risks):
            f.write(f"{pid},{rk:.6f}\n")

    console.print(f"\nCSV outputs saved:\n • {feat_path}\n • {risk_path}")

    # Simple text summary
    txt_path = os.path.join(results_dir, "summary.txt")
    with open(txt_path, 'w') as f:
        f.write(f"Best ensemble single-run c-index  : {best_single_c:.4f}\n")
        f.write(f"Bootstrap median c-index          : {c_median:.4f}\n")
        f.write(f"95 % confidence interval          : [{lo_:.4f}, {hi_:.4f}]\n")

    console.print("🎉  Pipeline complete.")

# ----------------------------------------------------------------------
# Entry-point
# ----------------------------------------------------------------------
if __name__ == "__main__":
    main()
